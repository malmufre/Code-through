---
title: "Code-Through"
author: "Marah Al Mufregh"
date: "07/10/2020"
output:
  html_document:
    df_print: paged
    theme: flatly
    highlight: haddock
    toc: yes
    toc_float: yes
---


# Welcome to my code-through

This code-through is an  introduction to text mining in R utilizing the text mining framework provided by the tm package.
We will also be using the wordcloud package to further visualize our findings.
The tm package was created by Ingo Feinerer. The package is considered quite new since it was published in 2019.
It enables people who are new to programming to easily analyze texts.
You can read more about the package [**here**](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf )


The purpose of this code-through is to understand how to analyze text ,  
find most frequent words used , and be able to
visualize the findings in a presentable manner.

The required packages for this code-through include `tm`, `dplyr`, `kableExtra`, `wordcloud`.

<br>



```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      Message=FALSE,
                      warning = FALSE)

library(tm)
library(dplyr)
library(kableExtra)
library(wordcloud)
```

<br>


## About the dataset

We will start by embedding a URL of a dataset from Yelp  which publishes crowd-sourced reviews about businesses.
The data is a detailed dump of Yelp reviews, businesses, users, and checkins for the Phoenix, AZ metropolitan area.
There are 229,907 yelp reviews in this dataset.
Since the dataset has many rows , we will reduce them to 7000 for ease of reference and easier loading into R.

The dataset is retrieved from [**data.world*](https://data.world/brianray/yelp-reviews ) .
The steps below are done to demonstrate how we can find the most frequent words 
used in reviews from the people depending on a particular business category.


*Check the steps below to load the dataset*



```{r results='hide'}

# copy and paste into a codeblock as follows

d <- read.csv("https://query.data.world/s/wmaxggxnhn4wi4jah3q73vjpvldaj6", header = TRUE,
    stringsAsFactors = FALSE, nrows = 7000)

```

```{r}
#Preview column names
colnames(d)
 
```

```{r results='hide'}
# Transfer all the column names to lower case.

colnames(d)<-tolower(colnames(d))
colnames(d)

```


```{r results='hide'}
summary(d)
```

<br>

Here we will choose required columns for our text analysis ,and make our dataset smaller and easier to understand.
This way we can view only these columns and disregard any extra columns that we do not need for our analysis.


The **text** section is what people have said about the place or in other words, they represent reviews on the place.
 
Since we are interested in finding most used words in relation to the business category , we will pick the following:

```{r results='hide'}

dat<-d[c( "business_name", "business_categories", "text" )]
dat
```

```{r results='hide'}
#View our new new dataset
View(dat)
```


*A preview of this dataset *

```{r}

head(dat)%>% kable() %>% kable_styling()

```



```{r}
summary(dat)
```


<br>


# Match our criteria

Here we will use the grep()  and grepl() functions to find business categories that match the criteria we specify and then it will return the full string.After that, we will use the grepl() which is the logical version of grep() , so it returns a vector of TRUE or FALSE, with TRUE representing the cases that match the specified criteria.

We will start by using `grep()` to match all the business categories that are considered to be in the coffee business.


```{r}
grep(pattern = "coffee", x = dat$business_categories, value = TRUE, ignore.case = TRUE) %>% head() %>%kable()
```
<br>

Here by using the `grepl()`function , we can get the count of  business categories that are considered to be in the coffee business within our 7000 rows.

We find that 262 places belong to the coffee category.

```{r}
coffee <- grepl("coffee", dat$business_categories, ignore.case = T)
sum(coffee)
```
<br>



We will use this criteria to find all places that belong to the coffee category.

```{r }
dat.cof<- dat[coffee, c( "business_name", "business_categories", "text")]
dat.cof %>% head(3) %>% kable() %>% kable_styling()
```



```{r results='hide'}
#New vector containing texts 
  cof.text<-dat.cof$text
  cof.text
```


<br>

# Load data as a corpus

Here we can load our data as a corpus.
We need to create a collection of documents (technically referred to as a Corpus). 
The `tm` package utilizes the Corpus as its main structure. A corpus is simply a collection of documents, but like most things in R, the corpus has specific attributes that enable certain types of analysis. 

```{r}

docs <- VCorpus(VectorSource(dat.cof$text))
docs

```



The output shows that this corpus has 262 documents

To Further explain this, we can check the object mycorp.
Notice how each string is treated as a document. 

```{r}
mycorp <- c("My name is Marah", "Her name is Sarah", "His name is John")
mycorp <- VCorpus(VectorSource(text))
mycorp
```
<br>


It is  necessary to identify a source of corpus. To see what sources are available for the tm package, try the function `getSources()` .

VectorSource a vector of characters (treats each component as a document).

```{r}
getSources()
```
<br>


# Corpus Transformation

What is considered useful in the  `tm` package is the ability to transform text into workable data without a great deal of code.
To do this, we can use Transformations which are accessible in the tm package. To see available Transformations enter getTransformations().


```{r}
getTransformations()

```



We can use the base R command writeLines() to write 2 lines of text number 2.I use this to double check that the transformations will be later applied to my data.
So far we do not have any transformations yet!

```{r}

writeLines(head(strwrap(docs[[2]]), 2)) 
```


<br>


Lets apply transformations!
The tm uses a specific interface to apply functions to corpora called `tm_map()`. Let's try it out.
Now we can remove  punctuation, numbers, make all words lower case, remove stop words and remove extra white space.

```{r results='hide'}
#Transform to lower case (need to wrap in content_transformer)
docs <- tm_map(docs, content_transformer(tolower))
```



```{r results='hide'}
# Since we are not interested in numbers because they do not contribute to meaning of text we will strip the digits
docs <- tm_map(docs, removeNumbers)
```

The next sage is to eliminate common words  from the text. These  incorporate words such as articles (a, an, the), conjunctions (and, or but etc.), common verbs (is), qualifiers (yet, however etc) . The tm package includes  a list of such stop words. We remove stop words using  removeWords transformation. 



```{r results='hide'}
docs <- tm_map(docs, removeWords, stopwords("english"))
```


```{r results='hide'}
#Remove punctuations
docs <- tm_map(docs, removePunctuation)
```

```{r results='hide'}
## Remove extra white spaces
docs <- tm_map(docs, stripWhitespace)

```



Usually a large corpus may have many words coming from the same root.
For Instance: Play, played , playing.
Stemming is the process of reducing  related words to their common root, which in this case would be the word play.

```{r}

stemmed.docs<- tm_map(docs, stemDocument)

# In this code-through we will not be stemming our docs

writeLines(head(strwrap(stemmed.docs[[2]]), 2)) 

```


*Note*:  If you go up the sheet to our previously loaded `writeLines()`function , you can see that the word **watch** was **watching** before stemming.



<br>



# Document term matrix
 
 The next step is to create a Document-Term Matrix (DTM). DTM is a matrix that lists all occurrences of words in the corpus. In a DTM, documents are represented by rows and the terms (or words) by columns. If a word occurs in a particular document n times, then the matrix entry for corresponding to that row and column is n, if it doesn't occur at all, the entry is 0.
 
```{r}



#Creates a term document matrix summary.
dtm <- TermDocumentMatrix(docs) 

#Unpack the summary into a matrix.
m <- as.matrix(dtm)

#Counts the words.
v <- sort(rowSums(m),decreasing=TRUE) 

#convert the count into a data frame.
d <- data.frame(word = names(v),freq=v)

#View top 15 words in terms of frequency.
head(d, 15)


 
```
<br>

# Visualize most frequent words

We will visualize the most used words using word clouds due to their simplicity in communicating qualitative findings in terms of words.


**Arguments of the word cloud generator function are**

* words : the words to be plotted
* freq : their frequencies
* min.freq : words with frequency below min.freq will not be plotted
* max.words : maximum number of words to be plotted
* random.order : plot words in random order. If false, they will be plotted in decreasing frequency
* rot.per : proportion words with 90 degree rotation (vertical text)
* colors : color words from least to most frequent. Use, for example, colors = black for single color.


```{r}

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=15, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

<br>

# Most frequent words in the travel business category


Here I will repeat the same steps for the sake of comparison. 
We will see the most frequent words used for business categories that fall under travel and compare them with the most frequent words used for coffeeshops.

```{r}

travel<- grepl("travel", dat$business_categories, ignore.case = T)
sum(travel)
```

```{r results='hide'}
dat.travel<- dat[travel, c( "business_name", "business_categories", "text")]
dat.travel %>% head(2) %>% kable() %>% kable_styling()
```

```{r results='hide'}
travel.text<-dat.travel$text
docs.travel <- VCorpus(VectorSource(dat.travel$text))

```

```{r results='hide'}
docs.travel <- tm_map(docs.travel, content_transformer(tolower))
docs.travel <- tm_map(docs.travel, removeNumbers)
docs.travel <- tm_map(docs.travel, removeWords, stopwords("english"))
docs.travel <- tm_map(docs.travel, removePunctuation)
docs.travel <- tm_map(docs.travel, stripWhitespace)

```


```{r}
dtm <- TermDocumentMatrix(docs.travel)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 15)
```


```{r}
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=15, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```



# Summary

As you can see , the most frequent used words to describe coffee shops are coffee, great , place etc..
All of these words represent reviews on the coffeeshops.
The most frequent words that describe business categories that fall under travel are room, hotel , service, resort etc..

I hope you enjoyed reading this and had the chance to understand more about the `tm`package.
Both the tm package and the wordcloud are considered great tools to perform quick and easy text analysis!


<br>



# Refrences & Further Resources

[**tm package**](https://cran.r-project.org/web/packages/tm/tm.pdf)

[**Text mining in data science**](https://medium.com/text-mining-in-data-science-a-tutorial-of-text/text-mining-in-data-science-51299e4e594)


There are also many other ways to perform text analysis in R.


If you are interested [**click here**](https://hub.packtpub.com/9-useful-r-packages-for-nlp-text-mining/)




